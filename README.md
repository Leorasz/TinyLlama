This project involved downloading the weights for Llama 3.1 8B Instruct from HuggingFace and then getting a working LLM without ever using the transformers library. This involved creating a transformer model in tinygrad which implemented some advanced practices like grouped query attention, RMS norms, and rotary position embeddings. It should be functional with similar Llama models as well because the hyperparameters are based off of a config file and not hard-coded, but I haven't tested it.

The goal of this project was to learn how LLMs are put together from files to words and also just get more familiar with transformers and modern additions to their architecture. Now that I have this set up I tear apart existing LLMs easier and make more things out of them like fine-tuning them or training them to do undo vector embeddings. 
